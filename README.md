# CrackIt AI

CrackIt AI is an AI-powered assistant designed to help students prepare for competitive entrance exams like JEE Main, JEE Advanced, NEET, BITSAT, and COMEDK. It provides a chat interface where users can ask questions related to specific subjects within these exams and receive answers generated by an AI model, supplemented with relevant past questions (PYQs) as context.

## Features

*   **Exam & Subject Selection:** Choose the specific exam and subject for targeted assistance.
*   **Chat Interface:** Ask questions and receive AI-generated answers in a familiar chat format.
*   **RAG Implementation:** Utilizes Retrieval-Augmented Generation (RAG) to find relevant past questions from a database and use them as context for the AI model.
*   **Context Display:** Shows the past questions used as context for the AI's answer.
*   **Question Details Modal:** View the full details (question, options, correct answer, explanation) of context questions in a modal window.
*   **LaTeX Support:** Renders mathematical formulas and scientific notation correctly using KaTeX.
*   **Chat History:** Saves chat conversations locally in the browser using LocalStorage.
*   **Chat Management:** Create, rename, and delete chat sessions.
*   **Responsive Design:** Basic styling for usability.

## Why CrackIt AI? (vs. Basic LLMs)

While standard Large Language Models (LLMs) possess vast general knowledge, they often lack the specific, up-to-date, and domain-focused information required for effective competitive exam preparation. CrackIt AI addresses this limitation through its Retrieval-Augmented Generation (RAG) approach:

*   **Contextual Relevance:** By retrieving relevant Past Year Questions (PYQs) related to your query, CrackIt AI grounds the LLM's response in actual exam content. This ensures answers are more specific and applicable to the chosen exam (JEE, NEET, etc.) and subject.
*   **Reduced Hallucination:** Standard LLMs can sometimes "hallucinate" or generate plausible but incorrect information. RAG mitigates this by forcing the LLM to base its answer on the provided PYQ context, leading to more accurate and reliable explanations.
*   **Domain Specificity:** The system leverages a curated database of PYQs, making the assistance highly specific to the patterns, difficulty levels, and topics frequently encountered in these competitive exams, something a general LLM might not capture accurately.
*   **Transparency:** Displaying the context questions used allows users to see the basis for the AI's answer and explore related problems.

In essence, CrackIt AI combines the generative power of LLMs with the factual accuracy and relevance of a specific exam question database, providing a more targeted and trustworthy study aid than a generic chatbot.

## Tech Stack

**Frontend:**

*   React (with Vite)
*   JavaScript (ES6+)
*   CSS (with some basic utility classes)
*   Axios (for API communication)
*   `react-latex-next` (for LaTeX rendering)
*   `katex`

**Backend:**

*   Python
*   FastAPI (web framework)
*   Google Generative AI (Gemini 2.0 Flash model)
*   Sentence Transformers (`all-MiniLM-L6-v2` for embeddings)
*   MongoDB (database for storing PYQs)
*   Scikit-learn (for cosine similarity)
*   Uvicorn (ASGI server)
*   Python-dotenv (for environment variables)
*   Pickle (for caching embeddings)

**Deployment:**

*   Backend configured for Vercel deployment (`vercel.json`).

## Project Structure

```
crackit-ai/
├── backend/
│   ├── api/
│   │   └── index.py        # FastAPI application logic
│   ├── env                 # Environment variables (ignored by git)
│   ├── generate_cache.py   # Script to generate embeddings cache
│   ├── qa_cache.pkl        # Pre-computed embeddings cache (ignored by git)
│   ├── requirements.txt    # Python dependencies
│   └── vercel.json         # Vercel deployment configuration
├── frontend/
│   ├── public/
│   │   └── logo.png
│   ├── src/
│   │   ├── components/     # React components (Sidebar, ChatArea, InputArea, etc.)
│   │   ├── utils/          # Utility functions (e.g., storage)
│   │   ├── App.jsx         # Main application component
│   │   ├── App.css         # Main application styles
│   │   ├── index.css       # Global styles
│   │   └── main.jsx        # Application entry point
│   ├── .eslintrc.js        # ESLint configuration
│   ├── index.html          # HTML entry point
│   ├── package.json        # Node.js dependencies and scripts
│   ├── vite.config.js      # Vite configuration
├── .gitignore              # Git ignore rules
└── README.md               # This file (Project root README)
```

## Setup and Running

**Prerequisites:**

*   Node.js and npm (or yarn)
*   Python 3.x and pip
*   MongoDB instance (local or cloud)
*   Google API Key for Generative AI

**Backend:**

1.  **Navigate to backend:** `cd backend`
2.  **Create virtual environment (optional but recommended):** `python -m venv env` and activate it (`source env/bin/activate` or `.\env\Scripts\activate`)
3.  **Install dependencies:** `pip install -r requirements.txt`
4.  **Set up environment variables:** Create a `.env` file in the `backend` directory with your `MONGODB_URI` and `GOOGLE_API_KEY`.
    ```.env
    MONGODB_URI=your_mongodb_connection_string
    GOOGLE_API_KEY=your_google_api_key
    ```
5.  **Populate MongoDB:** Ensure your MongoDB instance (specified in `MONGODB_URI`) has a database named `pyqs` and a collection named `questions` populated with the exam data. The data should include fields like `_id`, `question`, `options`, `correct_option`/`correct_value`, `explanation`, `exam` (ID), `subject` (ID).
6.  **Generate Embeddings Cache:** Run `python generate_cache.py`. This will connect to your local MongoDB (ensure it's running and accessible at `mongodb://localhost:27017` or update the script), process the questions, generate embeddings, and save them to `qa_cache.pkl`. *Note: The script currently connects to localhost; modify if your DB is elsewhere.*
7.  **Run the server:** `uvicorn api.index:app --reload --port 8000`

**Frontend:**

1.  **Navigate to frontend:** `cd frontend`
2.  **Install dependencies:** `npm install` (or `yarn install`)
3.  **Run the development server:** `npm run dev` (or `yarn dev`)
4.  Open your browser to `http://localhost:5173` (or the port specified by Vite).

## Usage

1.  Open the application in your browser.
2.  Create a new chat using the "New Chat" button in the sidebar.
3.  Select the desired Exam and Subject from the dropdowns above the input area.
4.  Type your question in the text area and press Enter or click the send button.
5.  The AI will respond, potentially using relevant past questions as context.
6.  Context questions are displayed below the AI's response. Click on a context question to view its full details in a modal.
7.  Manage your chats (rename, delete) using the icons next to the chat title in the sidebar.

## Backend RAG Process

1.  The `/ask` endpoint receives the user query, exam ID, subject ID, and `top_k`.
2.  It loads the pre-computed embeddings from `qa_cache.pkl`.
3.  Filters the cached embeddings based on the provided `exam_id` and `subject_id`.
4.  Encodes the user query into an embedding vector using the Sentence Transformer model.
5.  Calculates the cosine similarity between the query vector and the filtered document embeddings.
6.  Retrieves the `top_k` most similar documents (based on their IDs) from the MongoDB collection.
7.  Formats the retrieved documents (question and explanation) into a context string.
8.  Constructs a prompt containing the context and the user query.
9.  Sends the prompt to the Google Gemini API to generate an answer.
10. Returns the generated answer and the context documents used to the frontend.
